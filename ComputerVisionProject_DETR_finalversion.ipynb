{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Rq8qcpm-BQQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End Object Detection with TRansformers-DETR**"
      ],
      "metadata": {
        "id": "RJ_P-0asRdBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the COCO dataset"
      ],
      "metadata": {
        "id": "6F-3bkRDx3_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIbRqPgHx-Mo",
        "outputId": "c9e16d95-7f21-4c4f-dd59-6e455879970f",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-17 16:47:51--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.165.73, 52.217.236.41, 52.217.194.17, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.165.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘coco_train2017.zip’\n",
            "\n",
            "coco_train2017.zip  100%[===================>]  18.01G  19.4MB/s    in 16m 13s \n",
            "\n",
            "2024-07-17 17:04:05 (19.0 MB/s) - ‘coco_train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2024-07-17 17:04:05--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.165.179, 3.5.29.136, 54.231.224.1, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.165.179|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘coco_val2017.zip’\n",
            "\n",
            "coco_val2017.zip    100%[===================>] 777.80M  20.1MB/s    in 43s     \n",
            "\n",
            "2024-07-17 17:04:49 (18.0 MB/s) - ‘coco_val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2024-07-17 17:04:49--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.203.225, 52.216.26.68, 3.5.13.31, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.203.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘coco_ann2017.zip’\n",
            "\n",
            "coco_ann2017.zip    100%[===================>] 241.19M  18.0MB/s    in 15s     \n",
            "\n",
            "2024-07-17 17:05:04 (16.4 MB/s) - ‘coco_ann2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile, BadZipFile\n",
        "import os\n",
        "\n",
        "def extract_zip_file(extract_path, zip_path):\n",
        "    try:\n",
        "        with ZipFile(zip_path+\".zip\") as zfile:\n",
        "            zfile.extractall(extract_path)\n",
        "        # remove zipfile\n",
        "        zfileTOremove=f\"{zip_path}\"+\".zip\"\n",
        "        if os.path.isfile(zfileTOremove):\n",
        "            os.remove(zfileTOremove)\n",
        "        else:\n",
        "            print(\"Error: %s file not found\" % zfileTOremove)\n",
        "\n",
        "    except BadZipFile as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "extract_train_path = \"./coco_dataset_2017\"\n",
        "zip_train_path = \"./coco_train2017\"\n",
        "extract_val_path = \"./coco_dataset_2017\"\n",
        "zip_val_path = \"./coco_val2017\"\n",
        "extract_ann_path=\"./coco_dataset_2017\"\n",
        "zip_ann_path = \"./coco_ann2017\"\n",
        "\n",
        "extract_zip_file(extract_train_path, zip_train_path)\n",
        "extract_zip_file(extract_val_path, zip_val_path)\n",
        "extract_zip_file(extract_ann_path, zip_ann_path)"
      ],
      "metadata": {
        "id": "nPUVoD7UyX0C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Parser for the COCO-Dataset (not used)"
      ],
      "metadata": {
        "id": "dk2hG6qQTMKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class COCOParser:\n",
        "    def __init__(self, anns_file, imgs_dir):\n",
        "        with open(anns_file, 'r') as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.annIm_dict = defaultdict(list)\n",
        "        self.cat_dict = {}\n",
        "        self.annId_dict = {}\n",
        "        self.im_dict = {}\n",
        "        self.licenses_dict = {}\n",
        "\n",
        "        for ann in coco['annotations']:\n",
        "            self.annIm_dict[ann['image_id']].append(ann)\n",
        "            self.annId_dict[ann['id']]=ann\n",
        "        for img in coco['images']:\n",
        "            self.im_dict[img['id']] = img\n",
        "        for cat in coco['categories']:\n",
        "            self.cat_dict[cat['id']] = cat\n",
        "        for license in coco['licenses']:\n",
        "            self.licenses_dict[license['id']] = license\n",
        "\n",
        "    def get_imgIds(self):\n",
        "        return list(self.im_dict.keys())\n",
        "\n",
        "    def get_annIds(self, im_ids):\n",
        "        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "        return [ann['id'] for im_id in im_ids for ann in self.annIm_dict[im_id]]\n",
        "\n",
        "    def load_anns(self, ann_ids):\n",
        "        im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]\n",
        "        return [self.annId_dict[ann_id] for ann_id in ann_ids]\n",
        "\n",
        "    def load_cats(self, class_ids):\n",
        "        class_ids=class_ids if isinstance(class_ids, list) else [class_ids]\n",
        "        return [self.cat_dict[class_id] for class_id in class_ids]\n",
        "\n",
        "    def get_imgLicenses(self,im_ids):\n",
        "        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "        lic_ids = [self.im_dict[im_id][\"license\"] for im_id in im_ids]\n",
        "        return [self.licenses_dict[lic_id] for lic_id in lic_ids]\n",
        "  '''"
      ],
      "metadata": {
        "id": "4d2EEM-LzbGO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "fe665011-a6b9-4045-f37d-35b131d11121"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom collections import defaultdict\\nimport json\\nimport numpy as np\\n\\nclass COCOParser:\\n    def __init__(self, anns_file, imgs_dir):\\n        with open(anns_file, \\'r\\') as f:\\n            coco = json.load(f)\\n\\n        self.annIm_dict = defaultdict(list)\\n        self.cat_dict = {}\\n        self.annId_dict = {}\\n        self.im_dict = {}\\n        self.licenses_dict = {}\\n\\n        for ann in coco[\\'annotations\\']:\\n            self.annIm_dict[ann[\\'image_id\\']].append(ann)\\n            self.annId_dict[ann[\\'id\\']]=ann\\n        for img in coco[\\'images\\']:\\n            self.im_dict[img[\\'id\\']] = img\\n        for cat in coco[\\'categories\\']:\\n            self.cat_dict[cat[\\'id\\']] = cat\\n        for license in coco[\\'licenses\\']:\\n            self.licenses_dict[license[\\'id\\']] = license\\n\\n    def get_imgIds(self):\\n        return list(self.im_dict.keys())\\n\\n    def get_annIds(self, im_ids):\\n        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\\n        return [ann[\\'id\\'] for im_id in im_ids for ann in self.annIm_dict[im_id]]\\n\\n    def load_anns(self, ann_ids):\\n        im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]\\n        return [self.annId_dict[ann_id] for ann_id in ann_ids]\\n\\n    def load_cats(self, class_ids):\\n        class_ids=class_ids if isinstance(class_ids, list) else [class_ids]\\n        return [self.cat_dict[class_id] for class_id in class_ids]\\n\\n    def get_imgLicenses(self,im_ids):\\n        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\\n        lic_ids = [self.im_dict[im_id][\"license\"] for im_id in im_ids]\\n        return [self.licenses_dict[lic_id] for lic_id in lic_ids]\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "coco_annotations_file=\"/content/coco_ann2017/annotations/instances_val2017.json\"\n",
        "coco_images_dir=\"/content/coco_val2017/val2017\"\n",
        "coco= COCOParser(coco_annotations_file, coco_images_dir)\n",
        "'''"
      ],
      "metadata": {
        "id": "QlxlHbMl0ReH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d85ed098-9fe2-4c8b-8d3a-311496fac225"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncoco_annotations_file=\"/content/coco_ann2017/annotations/instances_val2017.json\"\\ncoco_images_dir=\"/content/coco_val2017/val2017\"\\ncoco= COCOParser(coco_annotations_file, coco_images_dir)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage of the dataset with simple Parser"
      ],
      "metadata": {
        "id": "m5NatqOs5cm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# define a list of colors for drawing bounding boxes\n",
        "color_list = [\"pink\", \"red\", \"teal\", \"blue\", \"orange\", \"yellow\", \"black\", \"magenta\",\"green\",\"aqua\"]*10\n",
        "\n",
        "num_imgs_to_disp = 4\n",
        "total_images = len(coco.get_imgIds()) # total number of images\n",
        "#print(f\"total_images: {total_images}\")\n",
        "sel_im_idxs = np.random.permutation(total_images)[:num_imgs_to_disp]\n",
        "#print(f\"sel_im_idxs: {sel_im_idxs}\")\n",
        "\n",
        "img_ids = coco.get_imgIds()\n",
        "selected_img_ids = [img_ids[i] for i in sel_im_idxs]\n",
        "#print(f\"selected_img_ids: {selected_img_ids}\")\n",
        "\n",
        "ann_ids = coco.get_annIds(selected_img_ids)\n",
        "im_licenses = coco.get_imgLicenses(selected_img_ids)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, im in enumerate(selected_img_ids):\n",
        "    #print(f\"i: {i}, im: {im}\")\n",
        "    image = Image.open(f\"{coco_images_dir}/{str(im).zfill(12)}.jpg\")\n",
        "    ann_ids = coco.get_annIds(im)\n",
        "    annotations = coco.load_anns(ann_ids)\n",
        "    for ann in annotations:\n",
        "        #print(f\"ann: {ann}\")\n",
        "        bbox = ann['bbox']\n",
        "        x, y, w, h = [int(b) for b in bbox]\n",
        "        class_id = ann[\"category_id\"]\n",
        "        class_name = coco.load_cats(class_id)[0][\"name\"]\n",
        "        license = coco.get_imgLicenses(im)[0][\"name\"]\n",
        "        color_ = color_list[class_id]\n",
        "        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color_, facecolor='none')\n",
        "\n",
        "        t_box=ax[i].text(x, y, class_name,  color='red', fontsize=10)\n",
        "        t_box.set_bbox(dict(boxstyle='square, pad=0',facecolor='white', alpha=0.6, edgecolor='blue'))\n",
        "        ax[i].add_patch(rect)\n",
        "\n",
        "\n",
        "    ax[i].axis('off')\n",
        "    ax[i].imshow(image)\n",
        "    ax[i].set_xlabel('Longitude')\n",
        "    ax[i].set_title(f\"License: {license}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "eK4hfnV413qX",
        "outputId": "f804ff51-b7e7-481e-8ceb-a0fd52585859"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\nimport numpy as np\\n\\n# define a list of colors for drawing bounding boxes\\ncolor_list = [\"pink\", \"red\", \"teal\", \"blue\", \"orange\", \"yellow\", \"black\", \"magenta\",\"green\",\"aqua\"]*10\\n\\nnum_imgs_to_disp = 4\\ntotal_images = len(coco.get_imgIds()) # total number of images\\n#print(f\"total_images: {total_images}\")\\nsel_im_idxs = np.random.permutation(total_images)[:num_imgs_to_disp]\\n#print(f\"sel_im_idxs: {sel_im_idxs}\")\\n\\nimg_ids = coco.get_imgIds()\\nselected_img_ids = [img_ids[i] for i in sel_im_idxs]\\n#print(f\"selected_img_ids: {selected_img_ids}\")\\n\\nann_ids = coco.get_annIds(selected_img_ids)\\nim_licenses = coco.get_imgLicenses(selected_img_ids)\\n\\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\\nax = ax.ravel()\\n\\nfor i, im in enumerate(selected_img_ids):\\n    #print(f\"i: {i}, im: {im}\")\\n    image = Image.open(f\"{coco_images_dir}/{str(im).zfill(12)}.jpg\")\\n    ann_ids = coco.get_annIds(im)\\n    annotations = coco.load_anns(ann_ids)\\n    for ann in annotations:\\n        #print(f\"ann: {ann}\")\\n        bbox = ann[\\'bbox\\']\\n        x, y, w, h = [int(b) for b in bbox]\\n        class_id = ann[\"category_id\"]\\n        class_name = coco.load_cats(class_id)[0][\"name\"]\\n        license = coco.get_imgLicenses(im)[0][\"name\"]\\n        color_ = color_list[class_id]\\n        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color_, facecolor=\\'none\\')\\n\\n        t_box=ax[i].text(x, y, class_name,  color=\\'red\\', fontsize=10)\\n        t_box.set_bbox(dict(boxstyle=\\'square, pad=0\\',facecolor=\\'white\\', alpha=0.6, edgecolor=\\'blue\\'))\\n        ax[i].add_patch(rect)\\n\\n\\n    ax[i].axis(\\'off\\')\\n    ax[i].imshow(image)\\n    ax[i].set_xlabel(\\'Longitude\\')\\n    ax[i].set_title(f\"License: {license}\")\\n\\nplt.tight_layout()\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CocoApi, taken from the paper"
      ],
      "metadata": {
        "id": "yb9gQOilJekg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transforms and data augmentation for both image + bbox.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "import PIL\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as Func\n",
        "\n",
        "\n",
        "def crop(image, target, region):\n",
        "    cropped_image = Func.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        target[\"area\"] = area\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        # FIXME should we update the area here if there are no boxes?\n",
        "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
        "        fields.append(\"masks\")\n",
        "\n",
        "    # remove elements for which the boxes or masks that have zero area\n",
        "    if \"boxes\" in target or \"masks\" in target:\n",
        "        # favor boxes selection when defining which elements to keep\n",
        "        # this is compatible with previous implementation\n",
        "        if \"boxes\" in target:\n",
        "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "        else:\n",
        "            keep = target['masks'].flatten(1).any(1)\n",
        "\n",
        "        for field in fields:\n",
        "            target[field] = target[field][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = Func.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # size can be min_size (scalar) or (w, h) tuple\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = Func.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    if \"area\" in target:\n",
        "        area = target[\"area\"]\n",
        "        scaled_area = area * (ratio_width * ratio_height)\n",
        "        target[\"area\"] = scaled_area\n",
        "\n",
        "    h, w = size\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = interpolate(\n",
        "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "\n",
        "def pad(image, target, padding):\n",
        "    # assumes that we only pad on the bottom right corners\n",
        "    padded_image = Func.pad(image, (0, 0, padding[0], padding[1]))\n",
        "    if target is None:\n",
        "        return padded_image, None\n",
        "    target = target.copy()\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
        "    return padded_image, target\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        region = T.RandomCrop.get_params(img, self.size)\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        image_width, image_height = img.size\n",
        "        crop_height, crop_width = self.size\n",
        "        crop_top = int(round((image_height - crop_height) / 2.))\n",
        "        crop_left = int(round((image_width - crop_width) / 2.))\n",
        "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "\n",
        "class RandomPad(object):\n",
        "    def __init__(self, max_pad):\n",
        "        self.max_pad = max_pad\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        pad_x = random.randint(0, self.max_pad)\n",
        "        pad_y = random.randint(0, self.max_pad)\n",
        "        return pad(img, target, (pad_x, pad_y))\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Randomly selects between transforms1 and transforms2,\n",
        "    with probability p for transforms1 and (1 - p) for transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return Func.to_tensor(img), target\n",
        "\n",
        "\n",
        "class RandomErasing(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        return self.eraser(img), target\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = Func.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = box_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string\n",
        "\n",
        "\"\"\"\n",
        "COCO dataset which returns image_id for evaluation.\n",
        "\n",
        "Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            RandomSelect(\n",
        "                RandomResize(scales, max_size=1333),\n",
        "                Compose([\n",
        "                    RandomResize([400, 500, 600]),\n",
        "                    RandomSizeCrop(384, 600),\n",
        "                    RandomResize(scales, max_size=1333),\n",
        "                ])\n",
        "            ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return Compose([\n",
        "            RandomResize([800], max_size=1333),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build_coco(image_set, args):\n",
        "    root = Path(args['coco_path'])\n",
        "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n",
        "        \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=False)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "\n",
        "\n",
        "def build_dataset(image_set, args):\n",
        "    dataset_file = args['dataset_file']\n",
        "    if dataset_file == 'coco':\n",
        "        return build_coco(image_set, args)\n",
        "    raise ValueError(f'dataset {dataset_file} not supported')"
      ],
      "metadata": {
        "id": "Ofo0QiUfgsRJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COCO evaluator that works in distributed mode.\n",
        "\n",
        "Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.py\n",
        "The difference is that there is less copy-pasting from pycocotools\n",
        "in the end of the file, as python3 can suppress prints with contextlib\n",
        "\"\"\"\n",
        "import os\n",
        "import contextlib\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "\n",
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = evaluate_1(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def evaluate_1(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "vmhPzFPaGqx-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some utils for the box operations"
      ],
      "metadata": {
        "id": "VEmjRiBUdGr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utilities for bounding box manipulation and GIoU.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torchvision.ops.boxes import box_area\n",
        "\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "# modified from torchvision to also return the union\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "\n",
        "def masks_to_boxes(masks):\n",
        "    \"\"\"Compute the bounding boxes around the provided masks\n",
        "\n",
        "    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n",
        "\n",
        "    Returns a [N, 4] tensors, with the boxes in xyxy format\n",
        "    \"\"\"\n",
        "    if masks.numel() == 0:\n",
        "        return torch.zeros((0, 4), device=masks.device)\n",
        "\n",
        "    h, w = masks.shape[-2:]\n",
        "\n",
        "    y = torch.arange(0, h, dtype=torch.float)\n",
        "    x = torch.arange(0, w, dtype=torch.float)\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    x_mask = (masks * x.unsqueeze(0))\n",
        "    x_max = x_mask.flatten(1).max(-1)[0]\n",
        "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    y_mask = (masks * y.unsqueeze(0))\n",
        "    y_max = y_mask.flatten(1).max(-1)[0]\n",
        "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
      ],
      "metadata": {
        "id": "pWga1LL0dE86"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here are defined some Classes and Functions including the **Nasted Tensors**\n",
        "Nasted Tensors are object composed of a tensor and a mask; they are used to compute features of the images. Actually the masks are used only for a computational purpose, therefore having the aim of making masked attentions.\n",
        "But in a possible modified architecture, masks can be used for computing segmentation of images.\n",
        "\n",
        "In **NastedTensor.tensor** there are batched images of shape [batch_size x 3 x H x W]\n",
        "\n",
        "In **NastedTensor.mask** there is a binary mask of shape [batch_size x H x W]\n",
        "\n",
        "There are also utility functions that will print information about training in an interactive way and give us the possibility to use a distributed version of the code that increase the computational power of the underlying machine"
      ],
      "metadata": {
        "id": "vPsO1qf1foI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from packaging import version\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "\n",
        "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
        "import torchvision\n",
        "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "    from torchvision.ops import _new_empty_tensor\n",
        "    from torchvision.ops.misc import _output_size\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "#\n",
        "# this kind of implementation speed-up the computation over nasted tensors\n",
        "#\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args['rank'] = int(os.environ[\"RANK\"])\n",
        "        args['world_size'] = int(os.environ['WORLD_SIZE'])\n",
        "        args['gpu'] = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args['rank'] = int(os.environ['SLURM_PROCID'])\n",
        "        args['gpu'] = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args['distributed'] = False\n",
        "        return\n",
        "\n",
        "    args['distributed'] = True\n",
        "\n",
        "    torch.cuda.set_device(args['gpu'])\n",
        "    args['dist_backend'] = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "               args['rank'], args['dist_url']), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args['dist_backend'],\n",
        "                                         init_method=args['dist_url'],\n",
        "                                         world_size=args['world_size'],\n",
        "                                         rank=args['rank'])\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args['rank'] == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)"
      ],
      "metadata": {
        "id": "eF_uAxQ8fxwK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Utilities"
      ],
      "metadata": {
        "id": "HSFYW3FeI64i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plotting utilities to visualize training logs.\n",
        "\"\"\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path, PurePath\n",
        "\n",
        "\n",
        "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt', type='train'):\n",
        "    '''\n",
        "    Function to plot specific fields from training log(s). Plots both training and test results.\n",
        "\n",
        "    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n",
        "              - fields = which results to plot from each log file - plots both training and test for each field.\n",
        "              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n",
        "              - log_name = optional, name of log file if different than default 'log.txt'.\n",
        "\n",
        "    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n",
        "               - solid lines are training results, dashed lines are test results.\n",
        "\n",
        "    '''\n",
        "    func_name = \"plot_utils.py::plot_logs\"\n",
        "\n",
        "    # verify logs is a list of Paths (list[Paths]) or single Pathlib object Path,\n",
        "    # convert single Path to list to avoid 'not iterable' error\n",
        "\n",
        "    if not isinstance(logs, list):\n",
        "        if isinstance(logs, PurePath):\n",
        "            logs = [logs]\n",
        "            print(f\"{func_name} info: logs param expects a list argument, converted to list[Path].\")\n",
        "        else:\n",
        "            raise ValueError(f\"{func_name} - invalid argument for logs parameter.\\n \\\n",
        "            Expect list[Path] or single Path obj, received {type(logs)}\")\n",
        "\n",
        "    # Quality checks - verify valid dir(s), that every item in list is Path object, and that log_name exists in each dir\n",
        "    for i, dir in enumerate(logs):\n",
        "        if not isinstance(dir, PurePath):\n",
        "            raise ValueError(f\"{func_name} - non-Path object in logs argument of {type(dir)}: \\n{dir}\")\n",
        "        if not dir.exists():\n",
        "            raise ValueError(f\"{func_name} - invalid directory in logs argument:\\n{dir}\")\n",
        "        # verify log_name exists\n",
        "        fn = Path(dir / log_name)\n",
        "        if not fn.exists():\n",
        "            print(f\"-> missing {log_name} file.  Have you gotten to Epoch 1 in training?\")\n",
        "            print(f\"--> full path of missing log file: {fn}\")\n",
        "            return\n",
        "\n",
        "    # load log file(s) and plot\n",
        "    dfs = [pd.read_json(Path(p) / log_name, lines=True) for p in logs]\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(fields), figsize=(16, 5))\n",
        "    fields_dict = {}\n",
        "    for df, color in zip(dfs, sns.color_palette(n_colors=len(logs))):\n",
        "        for j, field in enumerate(fields):\n",
        "            if field == 'mAP':\n",
        "                coco_eval = pd.DataFrame(\n",
        "                    np.stack(df.test_coco_eval_bbox.dropna().values)[:, 1]\n",
        "                ).ewm(com=ewm_col).mean()\n",
        "                axs[j].plot(coco_eval, c=color)\n",
        "            else:\n",
        "                # con pd.to_numeric mi prendo un vettore di 'field' lungo le epoche\n",
        "                train_elm = pd.to_numeric(df[f'train_{field}'], errors='coerce')\n",
        "                test_elm = pd.to_numeric(df[f'test_{field}'], errors='coerce')\n",
        "                transpose = [[row[i] for row in [train_elm, test_elm]] for i in range(len([train_elm, test_elm][0]))]\n",
        "                axs[j].plot(transpose)\n",
        "    for ax, field in zip(axs, fields):\n",
        "        ax.legend([Path(p).name for p in logs])\n",
        "        ax.set_title(field)\n",
        "\n",
        "\n",
        "def plot_precision_recall(files, naming_scheme='iter'):\n",
        "    if naming_scheme == 'exp_id':\n",
        "        # name becomes exp_id\n",
        "        names = [f.parts[-3] for f in files]\n",
        "    elif naming_scheme == 'iter':\n",
        "        names = [f.stem for f in files]\n",
        "    else:\n",
        "        raise ValueError(f'not supported {naming_scheme}')\n",
        "    fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n",
        "    for f, color, name in zip(files, sns.color_palette(\"Blues\", n_colors=len(files)), names):\n",
        "        data = torch.load(f)\n",
        "        # precision is n_iou, n_points, n_cat, n_area, max_det\n",
        "        precision = data['precision']\n",
        "        recall = data['params'].recThrs\n",
        "        scores = data['scores']\n",
        "        # take precision for all classes, all areas and 100 detections\n",
        "        precision = precision[0, :, :, 0, -1].mean(1)\n",
        "        scores = scores[0, :, :, 0, -1].mean(1)\n",
        "        prec = precision.mean()\n",
        "        rec = data['recall'][0, :, 0, -1].mean()\n",
        "        print(f'{naming_scheme} {name}: mAP@50={prec * 100: 05.1f}, ' +\n",
        "              f'score={scores.mean():0.3f}, ' +\n",
        "              f'f1={2 * prec * rec / (prec + rec + 1e-8):0.3f}'\n",
        "              )\n",
        "        axs[0].plot(recall, precision, c=color)\n",
        "        axs[1].plot(recall, scores, c=color)\n",
        "\n",
        "    axs[0].set_title('Precision / Recall')\n",
        "    axs[0].legend(names)\n",
        "    axs[1].set_title('Scores / Recall')\n",
        "    axs[1].legend(names)\n",
        "    return fig, axs"
      ],
      "metadata": {
        "id": "4ntVVGccJBXP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture\n",
        "Is the same one proposed in torch.nn.Transformer with some modifications:\n",
        "* positional encodings are passed in MHattention\n",
        "* extra LN at the end of encoder is removed\n",
        "* decoder returns a stack of activations from all decoding layers  \n",
        "\n",
        "First we will define the Encoder, next the Decoder and in the end the complete Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "3ooE3ZoZ5x-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some utility function for Encoder/Decoder layers:\n",
        "#\n",
        "# those utilities are used for having a dynamical number of encoders/decoders used in sequence\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "pVc7PLA80b9S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Architecture"
      ],
      "metadata": {
        "id": "zDrG0CotwYCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation_func=\"relu\", norm_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear_1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation_func)\n",
        "        self.normalize_before = norm_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout_1(src2)\n",
        "        src = self.norm_1(src)\n",
        "        src2 = self.linear_2(self.dropout(self.activation(self.linear_1(src))))\n",
        "        src = src + self.dropout_2(src2)\n",
        "        src = self.norm_2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm_1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout_1(src2)\n",
        "        src2 = self.norm_2(src)\n",
        "        src2 = self.linear_2(self.dropout(self.activation(self.linear_1(src2))))\n",
        "        src = src + self.dropout_2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.encoderLayers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for encoderLayer in self.encoderLayers:\n",
        "            output = encoderLayer(output, src_mask=mask,\n",
        "                          src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "I0IehckEwqFB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Architecture"
      ],
      "metadata": {
        "id": "tgjfZ8wIxyZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation_func=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear_1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.norm_3 = nn.LayerNorm(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation_func)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_1(tgt2)\n",
        "        tgt = self.norm_1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_2(tgt2)\n",
        "        tgt = self.norm_2(tgt)\n",
        "        tgt2 = self.linear_2(self.dropout(self.activation(self.linear_1(tgt))))\n",
        "        tgt = tgt + self.dropout_3(tgt2)\n",
        "        tgt = self.norm_3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm_1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_1(tgt2)\n",
        "        tgt2 = self.norm_2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_2(tgt2)\n",
        "        tgt2 = self.norm_3(tgt)\n",
        "        tgt2 = self.linear_2(self.dropout(self.activation(self.linear_1(tgt2))))\n",
        "        tgt = tgt + self.dropout_3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.decoderLayers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "\n",
        "        output = tgt\n",
        "\n",
        "        for decoderLayer in self.decoderLayers:\n",
        "            output = decoderLayer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "YWA8cRt3x2ac"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture"
      ],
      "metadata": {
        "id": "lKHsXv_NzYYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Transformer class.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # defining the type of encoder\n",
        "        encoder_Layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        if normalize_before:\n",
        "            encoder_norm = nn.LayerNorm(d_model)\n",
        "        else:\n",
        "            encoder_norm = None\n",
        "\n",
        "        # defining the block of ENCODERS\n",
        "        self.encoder_block = TransformerEncoder(encoder_Layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        # defining the type of decoder\n",
        "        decoder_Layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # defining the block of DECODERS\n",
        "        self.decoder_block = TransformerDecoder(decoder_Layer, num_decoder_layers,\n",
        "                                                decoder_norm,)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        # flatten: NxCxHxW -----> HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder_block(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        hs = self.decoder_block(tgt, memory, memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed, query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args['hidden_dim'],\n",
        "        dropout=args['dropout'],\n",
        "        nhead=args['nheads'],\n",
        "        dim_feedforward=args['dim_feedforward'],\n",
        "        num_encoder_layers=args['enc_layers'],\n",
        "        num_decoder_layers=args['dec_layers'],\n",
        "        normalize_before=args['pre_norm'],\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "lyEGJNYr52Ai"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the layer architecture, both for Encoder and Decoder, are implemented 2 ways to do the forward step:\n",
        "   * the first one is the *forward_post* where the Normalization is done after SAL/MHA/FFN layers\n",
        "   * the second one is the *forward_pre* where the Normalization is done before SAL/MHA/FFN layers\n",
        "\n",
        "In this way we can analyze different architectures and, by comparing them, analyze which one performs better"
      ],
      "metadata": {
        "id": "5AGs_7MX1PAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the Backbone part we need some utility functions, including the part that links the Backbone and the Transformer where there is also the positional embedding."
      ],
      "metadata": {
        "id": "9H1lXvu9VdqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding\n",
        "( used the one proposed in the paper )"
      ],
      "metadata": {
        "id": "zcpY8d0qXEoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            print(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=256):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = torch.cat([\n",
        "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(type_of_encoding, hidden_dim):\n",
        "    N_steps = hidden_dim // 2\n",
        "    if type_of_encoding in ('v2', 'sine'):\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif type_of_encoding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        print(f\"not supported {type_of_encoding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "m0ISuiN2XIm3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone Architecture\n",
        "ResNet18 architecture is used to perform the computation of image features"
      ],
      "metadata": {
        "id": "7Rq8qcpm-BQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone Architecture with FrozenBachNorm2d layers and with pretrained weights"
      ],
      "metadata": {
        "id": "8zGsgZ4hcv84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Backbone modules.\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict, List\n",
        "\n",
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str, train_backbone: bool, dilation: bool):\n",
        "        super().__init__()\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "                         replace_stride_with_dilation=[False, False, dilation],\n",
        "                         pretrained=is_main_process(),\n",
        "                         norm_layer=FrozenBatchNorm2d)\n",
        "\n",
        "        self.num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "\n",
        "        # the first layer is not trained, only the last 3 layers are trained\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "\n",
        "        # Eventually we can adapt the code to return all the intermediate outputs\n",
        "        # of the Backbone to look how features will change over layers\n",
        "        return_layers = {'layer4': \"0\"}\n",
        "\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self.body(tensor_list.tensors)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for layer, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            assert m is not None\n",
        "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            out[layer] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        featuresNTensors = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for layer, Ntensor in featuresNTensors.items():\n",
        "            out.append(Ntensor)\n",
        "            # position encoding\n",
        "            pos.append(self[1](Ntensor).to(Ntensor.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(backbone_args, hidden_dim):\n",
        "\n",
        "    position_embedding = build_position_encoding(backbone_args['type_of_encoding'], hidden_dim)\n",
        "\n",
        "    train_backbone = backbone_args['lr_backbone'] > 0\n",
        "\n",
        "    backbone = Backbone(backbone_args['type_of_backbone'], train_backbone,\n",
        "                        backbone_args['dilation'])\n",
        "    print(backbone)\n",
        "\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "id": "KSVwKsK0-HAr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DETR Model implementation"
      ],
      "metadata": {
        "id": "Uqw0vq-1e7pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used.\n",
        "            transformer: torch module of the transformer architecture.\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1) # to make backbone and transformer's size compatible\n",
        "        self.backbone = backbone\n",
        "        print(self.transformer)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZXo_nZEEmf5-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criterion\n",
        "this is a special class in which we compute the losses"
      ],
      "metadata": {
        "id": "rxbS3Kq7msEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, no_obj_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            no_obj_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.no_obj_coef = no_obj_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.no_obj_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "        # The distributed package included in PyTorch (i.e., torch.distributed)\n",
        "        # enables researchers and practitioners to easily parallelize their\n",
        "        # computations across processes and clusters of machines.\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        return losses"
      ],
      "metadata": {
        "id": "UrGVJUQwm9EH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PostProcess and MLP\n",
        "The module \"PostProcess\" converts the model's output into the format expected by the coco api, MLP is a very simple multi-layer perceptron (also called FFN)"
      ],
      "metadata": {
        "id": "Ppna8AcYnOJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PostProcess(nn.Module):\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation to adapt the output of the model to the input dimensions\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = F.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        # h = [input, hidden, ... , hidden, output]\n",
        "        h =[input_dim] + [hidden_dim] * (num_layers - 1) + [output_dim]\n",
        "        self.layers = nn.ModuleList(nn.Linear(h[i], h[i+1]) for i in range(len(h)-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.relu(layer(x))\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "V4R9hQ8jnbyr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hungarian Matcher\n",
        "Modules to compute the matching cost and solve the corresponding LSAP"
      ],
      "metadata": {
        "id": "ppZqIw-hncJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch import nn\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching between the model's output dimentions and the target dimentions\n",
        "\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is a vector containing the indices of the selected predictions (in order)\n",
        "                - index_j is a vector containing the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL(NegativeLogLikelihood),\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "\n",
        "        # Resizing the Cost matrix and computing the Hungarian assignment\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "\n",
        "def build_matcher(matcher_args):\n",
        "    return HungarianMatcher(cost_class=matcher_args['set_cost_class'],\n",
        "                            cost_bbox=matcher_args['set_cost_bbox'],\n",
        "                            cost_giou=matcher_args['set_cost_giou'])"
      ],
      "metadata": {
        "id": "lcwdC-cAqpwg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "tNKAB2HbroZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "\n",
        "    num_Classes = 20 if args[\"dataset_file\"] != 'coco' else 91\n",
        "\n",
        "    device = torch.device(args[\"device\"])\n",
        "\n",
        "    backbone = build_backbone(args[\"backbone\"], args[\"transformer\"]['hidden_dim'])\n",
        "\n",
        "    transformer = build_transformer(args[\"transformer\"])\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_Classes,\n",
        "        num_queries=args[\"num_queries\"],\n",
        "    )\n",
        "\n",
        "    Matcher = build_matcher(args[\"matcher\"])\n",
        "    Weight_dict = {'loss_ce': 1,\n",
        "                   'loss_bbox': args[\"bbox_loss_coef\"],\n",
        "                   'loss_giou' : args[\"giou_loss_coef\"]}\n",
        "\n",
        "\n",
        "    Losses = ['labels', 'boxes', 'cardinality']\n",
        "\n",
        "    criterion = SetCriterion(args[\"num_classes\"], matcher=Matcher, weight_dict=Weight_dict,\n",
        "                             no_obj_coef=args[\"no_obj_coef\"], losses=Losses)\n",
        "    criterion.to(device)\n",
        "    postprocessor = PostProcess()\n",
        "\n",
        "    return model, criterion, postprocessor"
      ],
      "metadata": {
        "id": "pgKpnobUR14x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "cYOs-NzsBQpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 1\n",
        "    i = 0\n",
        "\n",
        "    for samples, targets in data_loader:\n",
        "        if i > 9 :\n",
        "            break\n",
        "        if i == 0:\n",
        "            t, m = samples.decompose()\n",
        "            print(f\"samples in a batch: {t.shape[0]}\")\n",
        "        print(f\"batch n°{i}\")\n",
        "        i += 1\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            return {\"error\" : 1 }\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessor, data_loader, base_ds, device, output_dir):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "    postprocessors = {\"bbox\" : postprocessor}\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    i = 0\n",
        "\n",
        "    for samples, targets in data_loader:\n",
        "        if i > 0 :\n",
        "            break\n",
        "        print(f\"evaluating batch n°{i}\")\n",
        "        i += 1\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessor(outputs, orig_target_sizes)\n",
        "\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "    if coco_evaluator is not None:\n",
        "        stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "\n",
        "    return stats, coco_evaluator"
      ],
      "metadata": {
        "id": "5W0MOPkDF0wf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HYPER PARAMETERS**"
      ],
      "metadata": {
        "id": "m5cEcFtHTOOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = '/content/DETR_Output_Stats'\n",
        "os.makedirs(directory_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "jyKLCeSF3I7j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"backbone\" : {\n",
        "        'type_of_encoding' : 'sine',\n",
        "        'lr_backbone' : 0.0001,\n",
        "        'type_of_backbone' : 'resnet18',\n",
        "        'dilation' : False,\n",
        "    },\n",
        "    \"transformer\" : {\n",
        "        'hidden_dim' : 256,\n",
        "        'dropout' : 0.1,\n",
        "        'nheads' : 8,\n",
        "        'dim_feedforward' : 256,\n",
        "        'enc_layers' : 1,\n",
        "        'dec_layers' : 1,\n",
        "        'pre_norm' : True,\n",
        "    },\n",
        "    \"matcher\" : {\n",
        "        'set_cost_class' : 1,\n",
        "        'set_cost_bbox' : 5,\n",
        "        'set_cost_giou' : 2,\n",
        "    },\n",
        "    \"dataset_file\" : 'coco',\n",
        "    \"coco_path\" : '/content/coco_dataset_2017',\n",
        "    \"output_dir\" : '/content',\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"num_queries\" : 100,\n",
        "    \"num_classes\" : 90,\n",
        "    \"bbox_loss_coef\" : 5,\n",
        "    \"giou_loss_coef\" : 2,\n",
        "    \"no_obj_coef\" : 0.1,\n",
        "    \"batch_size\" : 4,#16,\n",
        "    \"eval\" : False,\n",
        "    \"num_workers\" : 2,\n",
        "    \"weight_decay\" : 1e-4,\n",
        "    \"lr\" : 1e-4,\n",
        "    'save_drop' : 2,\n",
        "    \"output_dir\" : '/content/DETR_Output_Stats',\n",
        "    \"dist_url\" : 'env://',\n",
        "    \"seed\" : 0,\n",
        "    \"resume\" : False,\n",
        "    \"clip_max_norm\" : 0.1 ,\n",
        "    \"start_epoch\" : 0 ,\n",
        "    \"epochs\" : 2 ,\n",
        "}\n",
        "\n",
        "init_distributed_mode(args)"
      ],
      "metadata": {
        "id": "0KDaHbx8Tci-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1605951f-2685-4fd8-b04d-17707f384051"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "\n",
        "init_distributed_mode(args)\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = args['seed'] + get_rank()\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "model, criterion, postprocessor = build(args)\n",
        "model.to(args['device'])\n",
        "\n",
        "model_without_ddp = model\n",
        "\n",
        "if args['distributed']:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=args['gpu'])\n",
        "    model_without_ddp = model.module\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "param_dicts = [\n",
        "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "    {\n",
        "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "        \"lr\": args['backbone']['lr_backbone'],\n",
        "    },\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_dicts, lr=args['lr'],\n",
        "                              weight_decay=args['weight_decay'])\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args['save_drop'])\n",
        "\n",
        "dataset_train = build_dataset(image_set='train', args=args)\n",
        "dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "if args['distributed']:\n",
        "    sampler_train = DistributedSampler(dataset_train)\n",
        "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "else:\n",
        "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train,\n",
        "                               args['batch_size'], drop_last=True)\n",
        "\n",
        "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                               collate_fn=collate_fn,\n",
        "                               num_workers=args['num_workers'])\n",
        "data_loader_val = DataLoader(dataset_val, 1, sampler=sampler_val,\n",
        "                             drop_last=False, collate_fn=collate_fn,\n",
        "                             num_workers=args['num_workers'])\n",
        "base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "output_dir = Path(args['output_dir'])\n",
        "\n",
        "#if args['resume']:\n",
        "#    if args['resume'].startswith('https'):\n",
        "#        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "#            args['resume'], map_location='cpu', check_hash=True)\n",
        "#    else:\n",
        "#        checkpoint = torch.load(args['resume'], map_location='cpu')\n",
        "#    model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "#    if not eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "#        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "#        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "#        start_epoch = checkpoint['epoch'] + 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WPi7k1vEIMa",
        "outputId": "704d7a6e-6a97-4611-e704-a1ff0371a904",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone(\n",
            "  (body): IntermediateLayerGetter(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): FrozenBatchNorm2d()\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Transformer(\n",
            "  (encoder_block): TransformerEncoder(\n",
            "    (encoderLayers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder_block): TransformerDecoder(\n",
            "    (decoderLayers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "number of params: 12379488\n",
            "loading annotations into memory...\n",
            "Done (t=21.04s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.63s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"(start_epoch, epochs): ({args['start_epoch']}, {args['epochs']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dTaxuv3bDUW",
        "outputId": "fbf761de-1c2d-40fc-b19c-36539ee23f06"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(start_epoch, epochs): (0, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if args['eval']:\n",
        "    print(\"\\nStart evaluating\")\n",
        "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessor,\n",
        "                          data_loader_val, base_ds, args['device'], output_dir)\n",
        "    if output_dir:\n",
        "        save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "else:\n",
        "  print(\"\\nStart training\")\n",
        "  start_time = time.time()\n",
        "  for epoch in range(args['start_epoch'], args['epochs']):\n",
        "      print(f\"\\n EPOCH n°{epoch}\")\n",
        "      if args['distributed']:\n",
        "          sampler_train.set_epoch(epoch)\n",
        "      train_stats = train_one_epoch(model, criterion, data_loader_train,\n",
        "                                    optimizer, args['device'], epoch,\n",
        "                                    args['clip_max_norm'])\n",
        "      lr_scheduler.step()\n",
        "      if output_dir:\n",
        "          checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "          # extra checkpoint before LR drop and every 100 epochs\n",
        "          if (epoch + 1) % args['save_drop'] == 0 or (epoch + 1) % 100 == 0:\n",
        "              checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "          for checkpoint_path in checkpoint_paths:\n",
        "              save_on_master({\n",
        "                  'model': model_without_ddp.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                  'epoch': epoch,\n",
        "                  'args': args,\n",
        "              }, checkpoint_path)\n",
        "\n",
        "      test_stats, coco_evaluator = evaluate(model, criterion, postprocessor,\n",
        "                                            data_loader_val, base_ds, args['device'],\n",
        "                                            output_dir)\n",
        "\n",
        "      log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                   **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                   'epoch': epoch,\n",
        "                   'n_parameters': n_parameters}\n",
        "\n",
        "      if output_dir and is_main_process():\n",
        "          with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "              f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "         # for evaluation logs\n",
        "          if coco_evaluator is not None:\n",
        "              (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "              if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                  filenames = ['latest.pth']\n",
        "                  if epoch % 50 == 0:\n",
        "                      filenames.append(f'{epoch:03}.pth')\n",
        "                  for name in filenames:\n",
        "                      torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                 output_dir / \"eval\" / name)\n",
        "\n",
        "  total_time = time.time() - start_time\n",
        "  total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "  print('Training time {}'.format(total_time_str))\n",
        "  update = args['epochs'] - args['start_epoch']\n",
        "  args['start_epoch'] += update\n",
        "  args['epochs'] += update\n"
      ],
      "metadata": {
        "id": "XawVfcdpBV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a582097d-88c5-4233-faaa-e12a6e6b2474"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start training\n",
            "\n",
            " EPOCH n°2\n",
            "samples in a batch: 4\n",
            "batch n°0\n",
            "batch n°1\n",
            "batch n°2\n",
            "batch n°3\n",
            "batch n°4\n",
            "batch n°5\n",
            "batch n°6\n",
            "batch n°7\n",
            "batch n°8\n",
            "batch n°9\n",
            "batch n°10\n",
            "batch n°11\n",
            "batch n°12\n",
            "batch n°13\n",
            "batch n°14\n",
            "batch n°15\n",
            "batch n°16\n",
            "batch n°17\n",
            "batch n°18\n",
            "batch n°19\n",
            "Averaged stats: lr: 0.000010  class_error: 100.00  loss: 6.5337 (6.6066)  loss_ce: 0.4176 (0.4906)  loss_bbox: 3.5325 (3.5700)  loss_giou: 2.6132 (2.5460)  loss_ce_unscaled: 0.4176 (0.4906)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7065 (0.7140)  loss_giou_unscaled: 1.3066 (1.2730)  cardinality_error_unscaled: 94.7500 (93.7625)\n",
            "evaluating batch n°0\n",
            "Averaged stats: class_error: 100.00  loss: 6.8192 (6.8192)  loss_ce: 1.5393 (1.5393)  loss_bbox: 2.6682 (2.6682)  loss_giou: 2.6116 (2.6116)  loss_ce_unscaled: 1.5393 (1.5393)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5336 (0.5336)  loss_giou_unscaled: 1.3058 (1.3058)  cardinality_error_unscaled: 80.0000 (80.0000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "\n",
            " EPOCH n°3\n",
            "samples in a batch: 4\n",
            "batch n°0\n",
            "batch n°1\n",
            "batch n°2\n",
            "batch n°3\n",
            "batch n°4\n",
            "batch n°5\n",
            "batch n°6\n",
            "batch n°7\n",
            "batch n°8\n",
            "batch n°9\n",
            "batch n°10\n",
            "batch n°11\n",
            "batch n°12\n",
            "batch n°13\n",
            "batch n°14\n",
            "batch n°15\n",
            "batch n°16\n",
            "batch n°17\n",
            "batch n°18\n",
            "batch n°19\n",
            "Averaged stats: lr: 0.000010  class_error: 100.00  loss: 6.3762 (6.3444)  loss_ce: 0.5117 (0.5020)  loss_bbox: 3.4050 (3.4185)  loss_giou: 2.4802 (2.4238)  loss_ce_unscaled: 0.5117 (0.5020)  class_error_unscaled: 100.0000 (99.8276)  loss_bbox_unscaled: 0.6810 (0.6837)  loss_giou_unscaled: 1.2401 (1.2119)  cardinality_error_unscaled: 92.7500 (93.5000)\n",
            "evaluating batch n°0\n",
            "Averaged stats: class_error: 100.00  loss: 6.9651 (6.9651)  loss_ce: 1.5549 (1.5549)  loss_bbox: 2.7919 (2.7919)  loss_giou: 2.6183 (2.6183)  loss_ce_unscaled: 1.5549 (1.5549)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5584 (0.5584)  loss_giou_unscaled: 1.3091 (1.3091)  cardinality_error_unscaled: 80.0000 (80.0000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Training time 0:10:36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "prova_dataset = CocoDetection(args['coco_path']+\"/val2017\",\n",
        "                              args['coco_path']+\"/annotations/instances_val2017.json\",\n",
        "                              transforms=make_coco_transforms(\"val\"),\n",
        "                              return_masks=False)\n",
        "\n",
        "print(f\"prova_dataset: {prova_dataset}\")\n",
        "print(f\"dataset_val: {dataset_val}\")\n",
        "# l'iterazione sul dataset funziona, adesso vediamo sul dataloader\n",
        "#for batch_ndx, sample in enumerate(prova_dataset):\n",
        "#    print(\" siamo dentro il for del batch di prova_dataset\")\n",
        "#    print(batch_ndx)\n",
        "#    #print(sample[0][0][0][0])\n",
        "\n",
        "sampler_prova = torch.utils.data.RandomSampler(prova_dataset)\n",
        "batch_sampler_prova = torch.utils.data.BatchSampler(sampler_prova,\n",
        "                               args['batch_size'], drop_last=True)\n",
        "data_loader_prova = DataLoader(prova_dataset, batch_sampler=batch_sampler_prova,\n",
        "                               collate_fn=collate_fn,\n",
        "                               num_workers=args['num_workers'])\n",
        "\n",
        "i = 0\n",
        "for sample_NT, target in data_loader_prova:\n",
        "    print(f\" siamo dentro il for del data_loader_prova: {i}\")\n",
        "    tens, mask = sample_NT.decompose()\n",
        "    print(f\"tens.shape: {tens.shape}\")\n",
        "    i = i+1\n",
        "'''"
      ],
      "metadata": {
        "id": "Q6FcM8bAi-bO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "ecafe22e-ee80-4146-c830-a0520842e30a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprova_dataset = CocoDetection(args[\\'coco_path\\']+\"/val2017\",\\n                              args[\\'coco_path\\']+\"/annotations/instances_val2017.json\",\\n                              transforms=make_coco_transforms(\"val\"),\\n                              return_masks=False)\\n\\nprint(f\"prova_dataset: {prova_dataset}\")\\nprint(f\"dataset_val: {dataset_val}\")\\n# l\\'iterazione sul dataset funziona, adesso vediamo sul dataloader\\n#for batch_ndx, sample in enumerate(prova_dataset):\\n#    print(\" siamo dentro il for del batch di prova_dataset\")\\n#    print(batch_ndx)\\n#    #print(sample[0][0][0][0])\\n\\nsampler_prova = torch.utils.data.RandomSampler(prova_dataset)\\nbatch_sampler_prova = torch.utils.data.BatchSampler(sampler_prova,\\n                               args[\\'batch_size\\'], drop_last=True)\\ndata_loader_prova = DataLoader(prova_dataset, batch_sampler=batch_sampler_prova,\\n                               collate_fn=collate_fn,\\n                               num_workers=args[\\'num_workers\\'])\\n\\ni = 0\\nfor sample_NT, target in data_loader_prova:\\n    print(f\" siamo dentro il for del data_loader_prova: {i}\")\\n    tens, mask = sample_NT.decompose()\\n    print(f\"tens.shape: {tens.shape}\")\\n    i = i+1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting log infos"
      ],
      "metadata": {
        "id": "jho69zgziRYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dir = Path(args['output_dir'])\n",
        "plot_logs([plot_dir],fields=('loss', 'loss_ce', 'loss_bbox', 'loss_giou')) #'class_error',"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "MKc02z5piRCs",
        "outputId": "6d7cad46-8a89-438d-c5e6-1f77ac3d52c4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO classes\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
      ],
      "metadata": {
        "id": "j4PER6OumW33"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}