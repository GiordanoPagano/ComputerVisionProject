{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Rq8qcpm-BQQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End Object Detection with TRansformers-DETR**"
      ],
      "metadata": {
        "id": "RJ_P-0asRdBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the COCO dataset"
      ],
      "metadata": {
        "id": "6F-3bkRDx3_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIbRqPgHx-Mo",
        "outputId": "cd1b581a-9889-4f4b-9918-a63aed2f9509",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-18 12:14:27--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.42.17, 52.217.162.41, 52.217.125.105, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.42.17|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘coco_train2017.zip’\n",
            "\n",
            "coco_train2017.zip  100%[===================>]  18.01G  20.1MB/s    in 16m 4s  \n",
            "\n",
            "2024-07-18 12:30:32 (19.1 MB/s) - ‘coco_train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2024-07-18 12:30:32--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.173.1, 52.216.43.209, 3.5.28.184, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.173.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘coco_val2017.zip’\n",
            "\n",
            "coco_val2017.zip    100%[===================>] 777.80M  10.7MB/s    in 76s     \n",
            "\n",
            "2024-07-18 12:31:48 (10.3 MB/s) - ‘coco_val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2024-07-18 12:31:48--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.129, 52.217.101.4, 52.217.128.113, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.129|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘coco_ann2017.zip’\n",
            "\n",
            "coco_ann2017.zip    100%[===================>] 241.19M  18.1MB/s    in 15s     \n",
            "\n",
            "2024-07-18 12:32:03 (16.3 MB/s) - ‘coco_ann2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile, BadZipFile\n",
        "import os\n",
        "\n",
        "def extract_zip_file(extract_path, zip_path):\n",
        "    try:\n",
        "        with ZipFile(zip_path+\".zip\") as zfile:\n",
        "            zfile.extractall(extract_path)\n",
        "        # remove zipfile\n",
        "        zfileTOremove=f\"{zip_path}\"+\".zip\"\n",
        "        if os.path.isfile(zfileTOremove):\n",
        "            os.remove(zfileTOremove)\n",
        "        else:\n",
        "            print(\"Error: %s file not found\" % zfileTOremove)\n",
        "\n",
        "    except BadZipFile as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "extract_train_path = \"./coco_dataset_2017\"\n",
        "zip_train_path = \"./coco_train2017\"\n",
        "extract_val_path = \"./coco_dataset_2017\"\n",
        "zip_val_path = \"./coco_val2017\"\n",
        "extract_ann_path=\"./coco_dataset_2017\"\n",
        "zip_ann_path = \"./coco_ann2017\"\n",
        "\n",
        "extract_zip_file(extract_train_path, zip_train_path)\n",
        "extract_zip_file(extract_val_path, zip_val_path)\n",
        "extract_zip_file(extract_ann_path, zip_ann_path)"
      ],
      "metadata": {
        "id": "nPUVoD7UyX0C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Parser for the COCO-Dataset (not used)"
      ],
      "metadata": {
        "id": "dk2hG6qQTMKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class COCOParser:\n",
        "    def __init__(self, anns_file, imgs_dir):\n",
        "        with open(anns_file, 'r') as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.annIm_dict = defaultdict(list)\n",
        "        self.cat_dict = {}\n",
        "        self.annId_dict = {}\n",
        "        self.im_dict = {}\n",
        "        self.licenses_dict = {}\n",
        "\n",
        "        for ann in coco['annotations']:\n",
        "            self.annIm_dict[ann['image_id']].append(ann)\n",
        "            self.annId_dict[ann['id']]=ann\n",
        "        for img in coco['images']:\n",
        "            self.im_dict[img['id']] = img\n",
        "        for cat in coco['categories']:\n",
        "            self.cat_dict[cat['id']] = cat\n",
        "        for license in coco['licenses']:\n",
        "            self.licenses_dict[license['id']] = license\n",
        "\n",
        "    def get_imgIds(self):\n",
        "        return list(self.im_dict.keys())\n",
        "\n",
        "    def get_annIds(self, im_ids):\n",
        "        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "        return [ann['id'] for im_id in im_ids for ann in self.annIm_dict[im_id]]\n",
        "\n",
        "    def load_anns(self, ann_ids):\n",
        "        im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]\n",
        "        return [self.annId_dict[ann_id] for ann_id in ann_ids]\n",
        "\n",
        "    def load_cats(self, class_ids):\n",
        "        class_ids=class_ids if isinstance(class_ids, list) else [class_ids]\n",
        "        return [self.cat_dict[class_id] for class_id in class_ids]\n",
        "\n",
        "    def get_imgLicenses(self,im_ids):\n",
        "        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\n",
        "        lic_ids = [self.im_dict[im_id][\"license\"] for im_id in im_ids]\n",
        "        return [self.licenses_dict[lic_id] for lic_id in lic_ids]\n",
        "  '''"
      ],
      "metadata": {
        "id": "4d2EEM-LzbGO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "e38882c2-be63-4747-bc2a-cc096efdb550"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom collections import defaultdict\\nimport json\\nimport numpy as np\\n\\nclass COCOParser:\\n    def __init__(self, anns_file, imgs_dir):\\n        with open(anns_file, \\'r\\') as f:\\n            coco = json.load(f)\\n\\n        self.annIm_dict = defaultdict(list)\\n        self.cat_dict = {}\\n        self.annId_dict = {}\\n        self.im_dict = {}\\n        self.licenses_dict = {}\\n\\n        for ann in coco[\\'annotations\\']:\\n            self.annIm_dict[ann[\\'image_id\\']].append(ann)\\n            self.annId_dict[ann[\\'id\\']]=ann\\n        for img in coco[\\'images\\']:\\n            self.im_dict[img[\\'id\\']] = img\\n        for cat in coco[\\'categories\\']:\\n            self.cat_dict[cat[\\'id\\']] = cat\\n        for license in coco[\\'licenses\\']:\\n            self.licenses_dict[license[\\'id\\']] = license\\n\\n    def get_imgIds(self):\\n        return list(self.im_dict.keys())\\n\\n    def get_annIds(self, im_ids):\\n        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\\n        return [ann[\\'id\\'] for im_id in im_ids for ann in self.annIm_dict[im_id]]\\n\\n    def load_anns(self, ann_ids):\\n        im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]\\n        return [self.annId_dict[ann_id] for ann_id in ann_ids]\\n\\n    def load_cats(self, class_ids):\\n        class_ids=class_ids if isinstance(class_ids, list) else [class_ids]\\n        return [self.cat_dict[class_id] for class_id in class_ids]\\n\\n    def get_imgLicenses(self,im_ids):\\n        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]\\n        lic_ids = [self.im_dict[im_id][\"license\"] for im_id in im_ids]\\n        return [self.licenses_dict[lic_id] for lic_id in lic_ids]\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "coco_annotations_file=\"/content/coco_ann2017/annotations/instances_val2017.json\"\n",
        "coco_images_dir=\"/content/coco_val2017/val2017\"\n",
        "coco= COCOParser(coco_annotations_file, coco_images_dir)\n",
        "'''"
      ],
      "metadata": {
        "id": "QlxlHbMl0ReH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "661a3d9b-aa67-41dc-8387-d01f2c012eff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncoco_annotations_file=\"/content/coco_ann2017/annotations/instances_val2017.json\"\\ncoco_images_dir=\"/content/coco_val2017/val2017\"\\ncoco= COCOParser(coco_annotations_file, coco_images_dir)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage of the dataset with simple Parser"
      ],
      "metadata": {
        "id": "m5NatqOs5cm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# define a list of colors for drawing bounding boxes\n",
        "color_list = [\"pink\", \"red\", \"teal\", \"blue\", \"orange\", \"yellow\", \"black\", \"magenta\",\"green\",\"aqua\"]*10\n",
        "\n",
        "num_imgs_to_disp = 4\n",
        "total_images = len(coco.get_imgIds()) # total number of images\n",
        "#print(f\"total_images: {total_images}\")\n",
        "sel_im_idxs = np.random.permutation(total_images)[:num_imgs_to_disp]\n",
        "#print(f\"sel_im_idxs: {sel_im_idxs}\")\n",
        "\n",
        "img_ids = coco.get_imgIds()\n",
        "selected_img_ids = [img_ids[i] for i in sel_im_idxs]\n",
        "#print(f\"selected_img_ids: {selected_img_ids}\")\n",
        "\n",
        "ann_ids = coco.get_annIds(selected_img_ids)\n",
        "im_licenses = coco.get_imgLicenses(selected_img_ids)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, im in enumerate(selected_img_ids):\n",
        "    #print(f\"i: {i}, im: {im}\")\n",
        "    image = Image.open(f\"{coco_images_dir}/{str(im).zfill(12)}.jpg\")\n",
        "    ann_ids = coco.get_annIds(im)\n",
        "    annotations = coco.load_anns(ann_ids)\n",
        "    for ann in annotations:\n",
        "        #print(f\"ann: {ann}\")\n",
        "        bbox = ann['bbox']\n",
        "        x, y, w, h = [int(b) for b in bbox]\n",
        "        class_id = ann[\"category_id\"]\n",
        "        class_name = coco.load_cats(class_id)[0][\"name\"]\n",
        "        license = coco.get_imgLicenses(im)[0][\"name\"]\n",
        "        color_ = color_list[class_id]\n",
        "        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color_, facecolor='none')\n",
        "\n",
        "        t_box=ax[i].text(x, y, class_name,  color='red', fontsize=10)\n",
        "        t_box.set_bbox(dict(boxstyle='square, pad=0',facecolor='white', alpha=0.6, edgecolor='blue'))\n",
        "        ax[i].add_patch(rect)\n",
        "\n",
        "\n",
        "    ax[i].axis('off')\n",
        "    ax[i].imshow(image)\n",
        "    ax[i].set_xlabel('Longitude')\n",
        "    ax[i].set_title(f\"License: {license}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "eK4hfnV413qX",
        "outputId": "4d416b1b-2ee1-41b9-8edc-35db7225ab22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\nimport numpy as np\\n\\n# define a list of colors for drawing bounding boxes\\ncolor_list = [\"pink\", \"red\", \"teal\", \"blue\", \"orange\", \"yellow\", \"black\", \"magenta\",\"green\",\"aqua\"]*10\\n\\nnum_imgs_to_disp = 4\\ntotal_images = len(coco.get_imgIds()) # total number of images\\n#print(f\"total_images: {total_images}\")\\nsel_im_idxs = np.random.permutation(total_images)[:num_imgs_to_disp]\\n#print(f\"sel_im_idxs: {sel_im_idxs}\")\\n\\nimg_ids = coco.get_imgIds()\\nselected_img_ids = [img_ids[i] for i in sel_im_idxs]\\n#print(f\"selected_img_ids: {selected_img_ids}\")\\n\\nann_ids = coco.get_annIds(selected_img_ids)\\nim_licenses = coco.get_imgLicenses(selected_img_ids)\\n\\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\\nax = ax.ravel()\\n\\nfor i, im in enumerate(selected_img_ids):\\n    #print(f\"i: {i}, im: {im}\")\\n    image = Image.open(f\"{coco_images_dir}/{str(im).zfill(12)}.jpg\")\\n    ann_ids = coco.get_annIds(im)\\n    annotations = coco.load_anns(ann_ids)\\n    for ann in annotations:\\n        #print(f\"ann: {ann}\")\\n        bbox = ann[\\'bbox\\']\\n        x, y, w, h = [int(b) for b in bbox]\\n        class_id = ann[\"category_id\"]\\n        class_name = coco.load_cats(class_id)[0][\"name\"]\\n        license = coco.get_imgLicenses(im)[0][\"name\"]\\n        color_ = color_list[class_id]\\n        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color_, facecolor=\\'none\\')\\n\\n        t_box=ax[i].text(x, y, class_name,  color=\\'red\\', fontsize=10)\\n        t_box.set_bbox(dict(boxstyle=\\'square, pad=0\\',facecolor=\\'white\\', alpha=0.6, edgecolor=\\'blue\\'))\\n        ax[i].add_patch(rect)\\n\\n\\n    ax[i].axis(\\'off\\')\\n    ax[i].imshow(image)\\n    ax[i].set_xlabel(\\'Longitude\\')\\n    ax[i].set_title(f\"License: {license}\")\\n\\nplt.tight_layout()\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CocoApi, taken from the paper"
      ],
      "metadata": {
        "id": "yb9gQOilJekg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transforms and data augmentation for both image + bbox.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "import PIL\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as Func\n",
        "\n",
        "\n",
        "def crop(image, target, region):\n",
        "    cropped_image = Func.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        target[\"area\"] = area\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        # FIXME should we update the area here if there are no boxes?\n",
        "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
        "        fields.append(\"masks\")\n",
        "\n",
        "    # remove elements for which the boxes or masks that have zero area\n",
        "    if \"boxes\" in target or \"masks\" in target:\n",
        "        # favor boxes selection when defining which elements to keep\n",
        "        # this is compatible with previous implementation\n",
        "        if \"boxes\" in target:\n",
        "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "        else:\n",
        "            keep = target['masks'].flatten(1).any(1)\n",
        "\n",
        "        for field in fields:\n",
        "            target[field] = target[field][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = Func.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # size can be min_size (scalar) or (w, h) tuple\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = Func.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    if \"area\" in target:\n",
        "        area = target[\"area\"]\n",
        "        scaled_area = area * (ratio_width * ratio_height)\n",
        "        target[\"area\"] = scaled_area\n",
        "\n",
        "    h, w = size\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = interpolate(\n",
        "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "\n",
        "def pad(image, target, padding):\n",
        "    # assumes that we only pad on the bottom right corners\n",
        "    padded_image = Func.pad(image, (0, 0, padding[0], padding[1]))\n",
        "    if target is None:\n",
        "        return padded_image, None\n",
        "    target = target.copy()\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
        "    return padded_image, target\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        region = T.RandomCrop.get_params(img, self.size)\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        image_width, image_height = img.size\n",
        "        crop_height, crop_width = self.size\n",
        "        crop_top = int(round((image_height - crop_height) / 2.))\n",
        "        crop_left = int(round((image_width - crop_width) / 2.))\n",
        "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "\n",
        "class RandomPad(object):\n",
        "    def __init__(self, max_pad):\n",
        "        self.max_pad = max_pad\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        pad_x = random.randint(0, self.max_pad)\n",
        "        pad_y = random.randint(0, self.max_pad)\n",
        "        return pad(img, target, (pad_x, pad_y))\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Randomly selects between transforms1 and transforms2,\n",
        "    with probability p for transforms1 and (1 - p) for transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return Func.to_tensor(img), target\n",
        "\n",
        "\n",
        "class RandomErasing(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        return self.eraser(img), target\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = Func.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = box_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string\n",
        "\n",
        "\"\"\"\n",
        "COCO dataset which returns image_id for evaluation.\n",
        "\n",
        "Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            RandomSelect(\n",
        "                RandomResize(scales, max_size=1333),\n",
        "                Compose([\n",
        "                    RandomResize([400, 500, 600]),\n",
        "                    RandomSizeCrop(384, 600),\n",
        "                    RandomResize(scales, max_size=1333),\n",
        "                ])\n",
        "            ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return Compose([\n",
        "            RandomResize([800], max_size=1333),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build_coco(image_set, args):\n",
        "    root = Path(args['coco_path'])\n",
        "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n",
        "        \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=False)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "\n",
        "\n",
        "def build_dataset(image_set, args):\n",
        "    dataset_file = args['dataset_file']\n",
        "    if dataset_file == 'coco':\n",
        "        return build_coco(image_set, args)\n",
        "    raise ValueError(f'dataset {dataset_file} not supported')"
      ],
      "metadata": {
        "id": "Ofo0QiUfgsRJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COCO evaluator that works in distributed mode.\n",
        "\n",
        "Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.py\n",
        "The difference is that there is less copy-pasting from pycocotools\n",
        "in the end of the file, as python3 can suppress prints with contextlib\n",
        "\"\"\"\n",
        "import os\n",
        "import contextlib\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "\n",
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = evaluate_1(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def evaluate_1(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "vmhPzFPaGqx-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some utils for the box operations"
      ],
      "metadata": {
        "id": "VEmjRiBUdGr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utilities for bounding box manipulation and GIoU.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torchvision.ops.boxes import box_area\n",
        "\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "# modified from torchvision to also return the union\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "\n",
        "def masks_to_boxes(masks):\n",
        "    \"\"\"Compute the bounding boxes around the provided masks\n",
        "\n",
        "    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n",
        "\n",
        "    Returns a [N, 4] tensors, with the boxes in xyxy format\n",
        "    \"\"\"\n",
        "    if masks.numel() == 0:\n",
        "        return torch.zeros((0, 4), device=masks.device)\n",
        "\n",
        "    h, w = masks.shape[-2:]\n",
        "\n",
        "    y = torch.arange(0, h, dtype=torch.float)\n",
        "    x = torch.arange(0, w, dtype=torch.float)\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    x_mask = (masks * x.unsqueeze(0))\n",
        "    x_max = x_mask.flatten(1).max(-1)[0]\n",
        "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    y_mask = (masks * y.unsqueeze(0))\n",
        "    y_max = y_mask.flatten(1).max(-1)[0]\n",
        "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
      ],
      "metadata": {
        "id": "pWga1LL0dE86"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here are defined some Classes and Functions including the **Nasted Tensors**\n",
        "Nasted Tensors are object composed of a tensor and a mask; they are used to compute features of the images. Actually the masks are used only for a computational purpose, therefore having the aim of making masked attentions.\n",
        "But in a possible modified architecture, masks can be used for computing segmentation of images.\n",
        "\n",
        "In **NastedTensor.tensor** there are batched images of shape [batch_size x 3 x H x W]\n",
        "\n",
        "In **NastedTensor.mask** there is a binary mask of shape [batch_size x H x W]\n",
        "\n",
        "There are also utility functions that will print information about training in an interactive way and give us the possibility to use a distributed version of the code that increase the computational power of the underlying machine"
      ],
      "metadata": {
        "id": "vPsO1qf1foI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from packaging import version\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "\n",
        "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
        "import torchvision\n",
        "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "    from torchvision.ops import _new_empty_tensor\n",
        "    from torchvision.ops.misc import _output_size\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "#\n",
        "# this kind of implementation speed-up the computation over nasted tensors\n",
        "#\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args['rank'] = int(os.environ[\"RANK\"])\n",
        "        args['world_size'] = int(os.environ['WORLD_SIZE'])\n",
        "        args['gpu'] = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args['rank'] = int(os.environ['SLURM_PROCID'])\n",
        "        args['gpu'] = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args['distributed'] = False\n",
        "        return\n",
        "\n",
        "    args['distributed'] = True\n",
        "\n",
        "    torch.cuda.set_device(args['gpu'])\n",
        "    args['dist_backend'] = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "               args['rank'], args['dist_url']), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args['dist_backend'],\n",
        "                                         init_method=args['dist_url'],\n",
        "                                         world_size=args['world_size'],\n",
        "                                         rank=args['rank'])\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args['rank'] == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)"
      ],
      "metadata": {
        "id": "eF_uAxQ8fxwK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Utilities"
      ],
      "metadata": {
        "id": "HSFYW3FeI64i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plotting utilities to visualize training logs.\n",
        "\"\"\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path, PurePath\n",
        "\n",
        "\n",
        "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt', type='train'):\n",
        "    '''\n",
        "    Function to plot specific fields from training log(s). Plots both training and test results.\n",
        "\n",
        "    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n",
        "              - fields = which results to plot from each log file - plots both training and test for each field.\n",
        "              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n",
        "              - log_name = optional, name of log file if different than default 'log.txt'.\n",
        "\n",
        "    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n",
        "               - solid lines are training results, dashed lines are test results.\n",
        "\n",
        "    '''\n",
        "    func_name = \"plot_utils.py::plot_logs\"\n",
        "\n",
        "    # verify logs is a list of Paths (list[Paths]) or single Pathlib object Path,\n",
        "    # convert single Path to list to avoid 'not iterable' error\n",
        "\n",
        "    if not isinstance(logs, list):\n",
        "        if isinstance(logs, PurePath):\n",
        "            logs = [logs]\n",
        "            print(f\"{func_name} info: logs param expects a list argument, converted to list[Path].\")\n",
        "        else:\n",
        "            raise ValueError(f\"{func_name} - invalid argument for logs parameter.\\n \\\n",
        "            Expect list[Path] or single Path obj, received {type(logs)}\")\n",
        "\n",
        "    # Quality checks - verify valid dir(s), that every item in list is Path object, and that log_name exists in each dir\n",
        "    for i, dir in enumerate(logs):\n",
        "        if not isinstance(dir, PurePath):\n",
        "            raise ValueError(f\"{func_name} - non-Path object in logs argument of {type(dir)}: \\n{dir}\")\n",
        "        if not dir.exists():\n",
        "            raise ValueError(f\"{func_name} - invalid directory in logs argument:\\n{dir}\")\n",
        "        # verify log_name exists\n",
        "        fn = Path(dir / log_name)\n",
        "        if not fn.exists():\n",
        "            print(f\"-> missing {log_name} file.  Have you gotten to Epoch 1 in training?\")\n",
        "            print(f\"--> full path of missing log file: {fn}\")\n",
        "            return\n",
        "\n",
        "    # load log file(s) and plot\n",
        "    dfs = [pd.read_json(Path(p) / log_name, lines=True) for p in logs]\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(fields), figsize=(16, 5))\n",
        "    fields_dict = {}\n",
        "    for df, color in zip(dfs, sns.color_palette(n_colors=len(logs))):\n",
        "        for j, field in enumerate(fields):\n",
        "            if field == 'mAP':\n",
        "                coco_eval = pd.DataFrame(\n",
        "                    np.stack(df.test_coco_eval_bbox.dropna().values)[:, 1]\n",
        "                ).ewm(com=ewm_col).mean()\n",
        "                axs[j].plot(coco_eval, c=color)\n",
        "            else:\n",
        "                # con pd.to_numeric mi prendo un vettore di 'field' lungo le epoche\n",
        "                train_elm = pd.to_numeric(df[f'train_{field}'], errors='coerce')\n",
        "                test_elm = pd.to_numeric(df[f'test_{field}'], errors='coerce')\n",
        "                transpose = [[row[i] for row in [train_elm, test_elm]] for i in range(len([train_elm, test_elm][0]))]\n",
        "                axs[j].plot(transpose, label=['train_elm', 'test_elm'])\n",
        "    for ax, field in zip(axs, fields):\n",
        "        ax.legend()#[Path(p).name for p in logs]\n",
        "        ax.set_title(field)\n",
        "\n",
        "\n",
        "def plot_precision_recall(files, naming_scheme='iter'):\n",
        "    if naming_scheme == 'exp_id':\n",
        "        # name becomes exp_id\n",
        "        names = [f.parts[-3] for f in files]\n",
        "    elif naming_scheme == 'iter':\n",
        "        names = [f.stem for f in files]\n",
        "    else:\n",
        "        raise ValueError(f'not supported {naming_scheme}')\n",
        "    fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n",
        "    for f, color, name in zip(files, sns.color_palette(\"Blues\", n_colors=len(files)), names):\n",
        "        data = torch.load(f)\n",
        "        # precision is n_iou, n_points, n_cat, n_area, max_det\n",
        "        precision = data['precision']\n",
        "        recall = data['params'].recThrs\n",
        "        scores = data['scores']\n",
        "        # take precision for all classes, all areas and 100 detections\n",
        "        precision = precision[0, :, :, 0, -1].mean(1)\n",
        "        scores = scores[0, :, :, 0, -1].mean(1)\n",
        "        prec = precision.mean()\n",
        "        rec = data['recall'][0, :, 0, -1].mean()\n",
        "        print(f'{naming_scheme} {name}: mAP@50={prec * 100: 05.1f}, ' +\n",
        "              f'score={scores.mean():0.3f}, ' +\n",
        "              f'f1={2 * prec * rec / (prec + rec + 1e-8):0.3f}'\n",
        "              )\n",
        "        axs[0].plot(recall, precision, c=color)\n",
        "        axs[1].plot(recall, scores, c=color)\n",
        "\n",
        "    axs[0].set_title('Precision / Recall')\n",
        "    axs[0].legend(names)\n",
        "    axs[1].set_title('Scores / Recall')\n",
        "    axs[1].legend(names)\n",
        "    return fig, axs"
      ],
      "metadata": {
        "id": "4ntVVGccJBXP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture\n",
        "Is the same one proposed in torch.nn.Transformer with some modifications:\n",
        "* positional encodings are passed in MHattention\n",
        "* extra LN at the end of encoder is removed\n",
        "* decoder returns a stack of activations from all decoding layers  \n",
        "\n",
        "First we will define the Encoder, next the Decoder and in the end the complete Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "3ooE3ZoZ5x-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some utility function for Encoder/Decoder layers:\n",
        "#\n",
        "# those utilities are used for having a dynamical number of encoders/decoders used in sequence\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "pVc7PLA80b9S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Architecture"
      ],
      "metadata": {
        "id": "zDrG0CotwYCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation_func=\"relu\", norm_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear_1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation_func)\n",
        "        self.normalize_before = norm_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout_1(src2)\n",
        "        src = self.norm_1(src)\n",
        "        src2 = self.linear_2(self.dropout(self.activation(self.linear_1(src))))\n",
        "        src = src + self.dropout_2(src2)\n",
        "        src = self.norm_2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm_1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout_1(src2)\n",
        "        src2 = self.norm_2(src)\n",
        "        src2 = self.linear_2(self.dropout(self.activation(self.linear_1(src2))))\n",
        "        src = src + self.dropout_2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.encoderLayers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for encoderLayer in self.encoderLayers:\n",
        "            output = encoderLayer(output, src_mask=mask,\n",
        "                          src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "I0IehckEwqFB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Architecture"
      ],
      "metadata": {
        "id": "tgjfZ8wIxyZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation_func=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear_1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.norm_3 = nn.LayerNorm(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation_func)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_1(tgt2)\n",
        "        tgt = self.norm_1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_2(tgt2)\n",
        "        tgt = self.norm_2(tgt)\n",
        "        tgt2 = self.linear_2(self.dropout(self.activation(self.linear_1(tgt))))\n",
        "        tgt = tgt + self.dropout_3(tgt2)\n",
        "        tgt = self.norm_3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm_1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_1(tgt2)\n",
        "        tgt2 = self.norm_2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout_2(tgt2)\n",
        "        tgt2 = self.norm_3(tgt)\n",
        "        tgt2 = self.linear_2(self.dropout(self.activation(self.linear_1(tgt2))))\n",
        "        tgt = tgt + self.dropout_3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.decoderLayers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "\n",
        "        output = tgt\n",
        "\n",
        "        for decoderLayer in self.decoderLayers:\n",
        "            output = decoderLayer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "YWA8cRt3x2ac"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture"
      ],
      "metadata": {
        "id": "lKHsXv_NzYYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Transformer class.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # defining the type of encoder\n",
        "        encoder_Layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        if normalize_before:\n",
        "            encoder_norm = nn.LayerNorm(d_model)\n",
        "        else:\n",
        "            encoder_norm = None\n",
        "\n",
        "        # defining the block of ENCODERS\n",
        "        self.encoder_block = TransformerEncoder(encoder_Layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        # defining the type of decoder\n",
        "        decoder_Layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # defining the block of DECODERS\n",
        "        self.decoder_block = TransformerDecoder(decoder_Layer, num_decoder_layers,\n",
        "                                                decoder_norm,)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        # flatten: NxCxHxW -----> HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder_block(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        hs = self.decoder_block(tgt, memory, memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed, query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args['hidden_dim'],\n",
        "        dropout=args['dropout'],\n",
        "        nhead=args['nheads'],\n",
        "        dim_feedforward=args['dim_feedforward'],\n",
        "        num_encoder_layers=args['enc_layers'],\n",
        "        num_decoder_layers=args['dec_layers'],\n",
        "        normalize_before=args['pre_norm'],\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "lyEGJNYr52Ai"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the layer architecture, both for Encoder and Decoder, are implemented 2 ways to do the forward step:\n",
        "   * the first one is the *forward_post* where the Normalization is done after SAL/MHA/FFN layers\n",
        "   * the second one is the *forward_pre* where the Normalization is done before SAL/MHA/FFN layers\n",
        "\n",
        "In this way we can analyze different architectures and, by comparing them, analyze which one performs better"
      ],
      "metadata": {
        "id": "5AGs_7MX1PAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the Backbone part we need some utility functions, including the part that links the Backbone and the Transformer where there is also the positional embedding."
      ],
      "metadata": {
        "id": "9H1lXvu9VdqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding\n",
        "( used the one proposed in the paper )"
      ],
      "metadata": {
        "id": "zcpY8d0qXEoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            print(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=256):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = torch.cat([\n",
        "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(type_of_encoding, hidden_dim):\n",
        "    N_steps = hidden_dim // 2\n",
        "    if type_of_encoding in ('v2', 'sine'):\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif type_of_encoding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        print(f\"not supported {type_of_encoding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "m0ISuiN2XIm3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone Architecture\n",
        "ResNet18 architecture is used to perform the computation of image features"
      ],
      "metadata": {
        "id": "7Rq8qcpm-BQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone Architecture with FrozenBachNorm2d layers and with pretrained weights"
      ],
      "metadata": {
        "id": "8zGsgZ4hcv84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Backbone modules.\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict, List\n",
        "\n",
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str, train_backbone: bool, dilation: bool):\n",
        "        super().__init__()\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "                         replace_stride_with_dilation=[False, False, dilation],\n",
        "                         pretrained=is_main_process(),\n",
        "                         norm_layer=FrozenBatchNorm2d)\n",
        "\n",
        "        self.num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "\n",
        "        # the first layer is not trained, only the last 3 layers are trained\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "\n",
        "        # Eventually we can adapt the code to return all the intermediate outputs\n",
        "        # of the Backbone to look how features will change over layers\n",
        "        return_layers = {'layer4': \"0\"}\n",
        "\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self.body(tensor_list.tensors)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for layer, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            assert m is not None\n",
        "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            out[layer] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        featuresNTensors = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for layer, Ntensor in featuresNTensors.items():\n",
        "            out.append(Ntensor)\n",
        "            # position encoding\n",
        "            pos.append(self[1](Ntensor).to(Ntensor.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(backbone_args, hidden_dim):\n",
        "\n",
        "    position_embedding = build_position_encoding(backbone_args['type_of_encoding'], hidden_dim)\n",
        "\n",
        "    train_backbone = backbone_args['lr_backbone'] > 0\n",
        "\n",
        "    backbone = Backbone(backbone_args['type_of_backbone'], train_backbone,\n",
        "                        backbone_args['dilation'])\n",
        "    print(backbone)\n",
        "\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "id": "KSVwKsK0-HAr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DETR Model implementation"
      ],
      "metadata": {
        "id": "Uqw0vq-1e7pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used.\n",
        "            transformer: torch module of the transformer architecture.\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1) # to make backbone and transformer's size compatible\n",
        "        self.backbone = backbone\n",
        "        print(self.transformer)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZXo_nZEEmf5-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criterion\n",
        "this is a special class in which we compute the losses"
      ],
      "metadata": {
        "id": "rxbS3Kq7msEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, no_obj_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            no_obj_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.no_obj_coef = no_obj_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.no_obj_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "        # The distributed package included in PyTorch (i.e., torch.distributed)\n",
        "        # enables researchers and practitioners to easily parallelize their\n",
        "        # computations across processes and clusters of machines.\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        return losses"
      ],
      "metadata": {
        "id": "UrGVJUQwm9EH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PostProcess and MLP\n",
        "The module \"PostProcess\" converts the model's output into the format expected by the coco api, MLP is a very simple multi-layer perceptron (also called FFN)"
      ],
      "metadata": {
        "id": "Ppna8AcYnOJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PostProcess(nn.Module):\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation to adapt the output of the model to the input dimensions\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = F.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        # h = [input, hidden, ... , hidden, output]\n",
        "        h =[input_dim] + [hidden_dim] * (num_layers - 1) + [output_dim]\n",
        "        self.layers = nn.ModuleList(nn.Linear(h[i], h[i+1]) for i in range(len(h)-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.relu(layer(x))\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "V4R9hQ8jnbyr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hungarian Matcher\n",
        "Modules to compute the matching cost and solve the corresponding LSAP"
      ],
      "metadata": {
        "id": "ppZqIw-hncJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch import nn\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching between the model's output dimentions and the target dimentions\n",
        "\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is a vector containing the indices of the selected predictions (in order)\n",
        "                - index_j is a vector containing the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL(NegativeLogLikelihood),\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "\n",
        "        # Resizing the Cost matrix and computing the Hungarian assignment\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "\n",
        "def build_matcher(matcher_args):\n",
        "    return HungarianMatcher(cost_class=matcher_args['set_cost_class'],\n",
        "                            cost_bbox=matcher_args['set_cost_bbox'],\n",
        "                            cost_giou=matcher_args['set_cost_giou'])"
      ],
      "metadata": {
        "id": "lcwdC-cAqpwg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "tNKAB2HbroZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "\n",
        "    num_Classes = 20 if args[\"dataset_file\"] != 'coco' else 91\n",
        "\n",
        "    device = torch.device(args[\"device\"])\n",
        "\n",
        "    backbone = build_backbone(args[\"backbone\"], args[\"transformer\"]['hidden_dim'])\n",
        "\n",
        "    transformer = build_transformer(args[\"transformer\"])\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_Classes,\n",
        "        num_queries=args[\"num_queries\"],\n",
        "    )\n",
        "\n",
        "    Matcher = build_matcher(args[\"matcher\"])\n",
        "    Weight_dict = {'loss_ce': 1,\n",
        "                   'loss_bbox': args[\"bbox_loss_coef\"],\n",
        "                   'loss_giou' : args[\"giou_loss_coef\"]}\n",
        "\n",
        "\n",
        "    Losses = ['labels', 'boxes', 'cardinality']\n",
        "\n",
        "    criterion = SetCriterion(args[\"num_classes\"], matcher=Matcher, weight_dict=Weight_dict,\n",
        "                             no_obj_coef=args[\"no_obj_coef\"], losses=Losses)\n",
        "    criterion.to(device)\n",
        "    postprocessor = PostProcess()\n",
        "\n",
        "    return model, criterion, postprocessor"
      ],
      "metadata": {
        "id": "pgKpnobUR14x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "cYOs-NzsBQpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 1\n",
        "    i = 0\n",
        "\n",
        "    for samples, targets in data_loader:\n",
        "        if i > 9 :\n",
        "            break\n",
        "        if i == 0:\n",
        "            t, m = samples.decompose()\n",
        "            print(f\"samples in a batch: {t.shape[0]}\")\n",
        "        print(f\"batch n°{i}\")\n",
        "        i += 1\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            return {\"error\" : 1 }\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessor, data_loader, base_ds, device, output_dir):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "    postprocessors = {\"bbox\" : postprocessor}\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    i = 0\n",
        "\n",
        "    for samples, targets in data_loader:\n",
        "        if i > 0 :\n",
        "            break\n",
        "        print(f\"evaluating batch n°{i}\")\n",
        "        i += 1\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessor(outputs, orig_target_sizes)\n",
        "\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "    if coco_evaluator is not None:\n",
        "        stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "\n",
        "    return stats, coco_evaluator"
      ],
      "metadata": {
        "id": "5W0MOPkDF0wf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HYPER PARAMETERS**"
      ],
      "metadata": {
        "id": "m5cEcFtHTOOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = '/content/DETR_Output_Stats'\n",
        "os.makedirs(directory_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "jyKLCeSF3I7j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"backbone\" : {\n",
        "        'type_of_encoding' : 'sine',\n",
        "        'lr_backbone' : 0.0001,\n",
        "        'type_of_backbone' : 'resnet18',\n",
        "        'dilation' : False,\n",
        "    },\n",
        "    \"transformer\" : {\n",
        "        'hidden_dim' : 256,\n",
        "        'dropout' : 0.1,\n",
        "        'nheads' : 8,\n",
        "        'dim_feedforward' : 256,\n",
        "        'enc_layers' : 1,\n",
        "        'dec_layers' : 1,\n",
        "        'pre_norm' : True,\n",
        "    },\n",
        "    \"matcher\" : {\n",
        "        'set_cost_class' : 1,\n",
        "        'set_cost_bbox' : 5,\n",
        "        'set_cost_giou' : 2,\n",
        "    },\n",
        "    \"dataset_file\" : 'coco',\n",
        "    \"coco_path\" : '/content/coco_dataset_2017',\n",
        "    \"output_dir\" : '/content/DETR_Output_Stats',\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"num_queries\" : 100,\n",
        "    \"num_classes\" : 90,\n",
        "    \"bbox_loss_coef\" : 5,\n",
        "    \"giou_loss_coef\" : 2,\n",
        "    \"no_obj_coef\" : 0.1,\n",
        "    \"batch_size\" : 4,#16,\n",
        "    \"eval\" : False,\n",
        "    \"num_workers\" : 2,\n",
        "    \"weight_decay\" : 1e-4,\n",
        "    \"lr\" : 1e-4,\n",
        "    'save_drop' : 2,\n",
        "    \"dist_url\" : 'env://',\n",
        "    \"seed\" : 0,\n",
        "    \"resume\" : False,\n",
        "    \"clip_max_norm\" : 0.1 ,\n",
        "    \"start_epoch\" : 0 ,\n",
        "    \"epochs\" : 2 ,\n",
        "}\n",
        "\n",
        "init_distributed_mode(args)"
      ],
      "metadata": {
        "id": "0KDaHbx8Tci-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd7327c-54c6-44b5-91c6-821dc9cd0ae8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "\n",
        "init_distributed_mode(args)\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = args['seed'] + get_rank()\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "#model, criterion, postprocessor = build(args)\n",
        "model.to(args['device'])\n",
        "\n",
        "model_without_ddp = model\n",
        "\n",
        "if args['distributed']:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=args['gpu'])\n",
        "    model_without_ddp = model.module\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "param_dicts = [\n",
        "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "    {\n",
        "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "        \"lr\": args['backbone']['lr_backbone'],\n",
        "    },\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_dicts, lr=args['lr'],\n",
        "                              weight_decay=args['weight_decay'])\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args['save_drop'])\n",
        "\n",
        "dataset_train = build_dataset(image_set='train', args=args)\n",
        "dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "if args['distributed']:\n",
        "    sampler_train = DistributedSampler(dataset_train)\n",
        "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "else:\n",
        "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train,\n",
        "                               args['batch_size'], drop_last=True)\n",
        "\n",
        "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                               collate_fn=collate_fn,\n",
        "                               num_workers=args['num_workers'])\n",
        "data_loader_val = DataLoader(dataset_val, 1, sampler=sampler_val,\n",
        "                             drop_last=False, collate_fn=collate_fn,\n",
        "                             num_workers=args['num_workers'])\n",
        "base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "output_dir = Path(args['output_dir'])\n",
        "\n",
        "#if args['resume']:\n",
        "#    if args['resume'].startswith('https'):\n",
        "#        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "#            args['resume'], map_location='cpu', check_hash=True)\n",
        "#    else:\n",
        "#        checkpoint = torch.load(args['resume'], map_location='cpu')\n",
        "#    model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "#    if not eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "#        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "#        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "#        start_epoch = checkpoint['epoch'] + 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WPi7k1vEIMa",
        "outputId": "a1910f70-6d96-410c-c9f5-d3d156e7b282",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 48.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone(\n",
            "  (body): IntermediateLayerGetter(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): FrozenBatchNorm2d()\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): FrozenBatchNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): FrozenBatchNorm2d()\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): FrozenBatchNorm2d()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Transformer(\n",
            "  (encoder_block): TransformerEncoder(\n",
            "    (encoderLayers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder_block): TransformerDecoder(\n",
            "    (decoderLayers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear_1): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "number of params: 12379488\n",
            "loading annotations into memory...\n",
            "Done (t=21.85s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.66s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"(start_epoch, epochs): ({args['start_epoch']}, {args['epochs']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dTaxuv3bDUW",
        "outputId": "803db110-b0c9-4722-91da-f8544e138b90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(start_epoch, epochs): (0, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if args['eval']:\n",
        "    print(\"\\nStart evaluating\")\n",
        "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessor,\n",
        "                          data_loader_val, base_ds, args['device'], output_dir)\n",
        "    if output_dir:\n",
        "        save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "else:\n",
        "  print(\"\\nStart training\")\n",
        "  start_time = time.time()\n",
        "  for epoch in range(args['start_epoch'], args['epochs']):\n",
        "      print(f\"\\n EPOCH n°{epoch}\")\n",
        "      if args['distributed']:\n",
        "          sampler_train.set_epoch(epoch)\n",
        "      train_stats = train_one_epoch(model, criterion, data_loader_train,\n",
        "                                    optimizer, args['device'], epoch,\n",
        "                                    args['clip_max_norm'])\n",
        "      lr_scheduler.step()\n",
        "      if output_dir:\n",
        "          checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "          # extra checkpoint before LR drop and every 100 epochs\n",
        "          if (epoch + 1) % args['save_drop'] == 0 or (epoch + 1) % 100 == 0:\n",
        "              checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "          for checkpoint_path in checkpoint_paths:\n",
        "              save_on_master({\n",
        "                  'model': model_without_ddp.state_dict(),\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                  'epoch': epoch,\n",
        "                  'args': args,\n",
        "              }, checkpoint_path)\n",
        "\n",
        "      test_stats, coco_evaluator = evaluate(model, criterion, postprocessor,\n",
        "                                            data_loader_val, base_ds, args['device'],\n",
        "                                            output_dir)\n",
        "\n",
        "      log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                   **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                   'epoch': epoch,\n",
        "                   'n_parameters': n_parameters}\n",
        "\n",
        "      if output_dir and is_main_process():\n",
        "          with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "              f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "         # for evaluation logs\n",
        "          if coco_evaluator is not None:\n",
        "              (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "              if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                  filenames = ['latest.pth']\n",
        "                  if epoch % 50 == 0:\n",
        "                      filenames.append(f'{epoch:03}.pth')\n",
        "                  for name in filenames:\n",
        "                      torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                 output_dir / \"eval\" / name)\n",
        "\n",
        "  total_time = time.time() - start_time\n",
        "  total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "  print('Training time {}'.format(total_time_str))\n",
        "  update = args['epochs'] - args['start_epoch']\n",
        "  args['start_epoch'] += update\n",
        "  args['epochs'] += update\n",
        "\n",
        "  plot_logs([Path(args['output_dir'])],fields=('class_error','loss', 'loss_ce', 'loss_bbox', 'loss_giou'))\n"
      ],
      "metadata": {
        "id": "XawVfcdpBV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a25b730-4d55-437d-fe5b-f6e2be6ad99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start training\n",
            "\n",
            " EPOCH n°14\n",
            "samples in a batch: 4\n",
            "batch n°0\n",
            "batch n°1\n",
            "batch n°2\n",
            "batch n°3\n",
            "batch n°4\n",
            "batch n°5\n",
            "batch n°6\n",
            "batch n°7\n",
            "batch n°8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "prova_dataset = CocoDetection(args['coco_path']+\"/val2017\",\n",
        "                              args['coco_path']+\"/annotations/instances_val2017.json\",\n",
        "                              transforms=make_coco_transforms(\"val\"),\n",
        "                              return_masks=False)\n",
        "\n",
        "print(f\"prova_dataset: {prova_dataset}\")\n",
        "print(f\"dataset_val: {dataset_val}\")\n",
        "# l'iterazione sul dataset funziona, adesso vediamo sul dataloader\n",
        "#for batch_ndx, sample in enumerate(prova_dataset):\n",
        "#    print(\" siamo dentro il for del batch di prova_dataset\")\n",
        "#    print(batch_ndx)\n",
        "#    #print(sample[0][0][0][0])\n",
        "\n",
        "sampler_prova = torch.utils.data.RandomSampler(prova_dataset)\n",
        "batch_sampler_prova = torch.utils.data.BatchSampler(sampler_prova,\n",
        "                               args['batch_size'], drop_last=True)\n",
        "data_loader_prova = DataLoader(prova_dataset, batch_sampler=batch_sampler_prova,\n",
        "                               collate_fn=collate_fn,\n",
        "                               num_workers=args['num_workers'])\n",
        "\n",
        "i = 0\n",
        "for sample_NT, target in data_loader_prova:\n",
        "    print(f\" siamo dentro il for del data_loader_prova: {i}\")\n",
        "    tens, mask = sample_NT.decompose()\n",
        "    print(f\"tens.shape: {tens.shape}\")\n",
        "    i = i+1\n",
        "'''"
      ],
      "metadata": {
        "id": "Q6FcM8bAi-bO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "8438ba82-f16a-48df-f27b-878cc6495d34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprova_dataset = CocoDetection(args[\\'coco_path\\']+\"/val2017\",\\n                              args[\\'coco_path\\']+\"/annotations/instances_val2017.json\",\\n                              transforms=make_coco_transforms(\"val\"),\\n                              return_masks=False)\\n\\nprint(f\"prova_dataset: {prova_dataset}\")\\nprint(f\"dataset_val: {dataset_val}\")\\n# l\\'iterazione sul dataset funziona, adesso vediamo sul dataloader\\n#for batch_ndx, sample in enumerate(prova_dataset):\\n#    print(\" siamo dentro il for del batch di prova_dataset\")\\n#    print(batch_ndx)\\n#    #print(sample[0][0][0][0])\\n\\nsampler_prova = torch.utils.data.RandomSampler(prova_dataset)\\nbatch_sampler_prova = torch.utils.data.BatchSampler(sampler_prova,\\n                               args[\\'batch_size\\'], drop_last=True)\\ndata_loader_prova = DataLoader(prova_dataset, batch_sampler=batch_sampler_prova,\\n                               collate_fn=collate_fn,\\n                               num_workers=args[\\'num_workers\\'])\\n\\ni = 0\\nfor sample_NT, target in data_loader_prova:\\n    print(f\" siamo dentro il for del data_loader_prova: {i}\")\\n    tens, mask = sample_NT.decompose()\\n    print(f\"tens.shape: {tens.shape}\")\\n    i = i+1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting log infos"
      ],
      "metadata": {
        "id": "jho69zgziRYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dir = Path(args['output_dir'])\n",
        "plot_logs([plot_dir],fields=('loss', 'loss_ce', 'loss_bbox', 'loss_giou')) #'class_error',"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "MKc02z5piRCs",
        "outputId": "8fc0392d-a3b2-4e80-8079-9c49489dd767"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO classes\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
      ],
      "metadata": {
        "id": "j4PER6OumW33"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}
